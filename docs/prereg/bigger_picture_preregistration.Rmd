---
title           : "Seeing the Bigger Picture: Exploring Children's Screen Time and Outcomes through Collaborative Data Analysis"
shorttitle      : "Bigger Picture"

author: 
  - name        : Taren Sanders
    affiliation : 1
  - name        : James Conigrave
    affiliation : 2
  - name        : Michael Noetel
    affiliation : 3
  - name        : Rebecca Pagano
    affiliation : 1
  - name        : Chloe Gordon
    affiliation : 1
  - name        : Bridget Booker
    affiliation : 1
  - name        : Chris Lonsdale
    affiliation : 1
  - name        : Aliza Werner-Seidler
    affiliation : 4
  - name        : Leon Straker
    affiliation : 5
  - name        : Dylan Cliff
    affiliation : 6

affiliation:
  - id          : 1
    institution : Australian Catholic University
  - id          : 2
    institution : La Trobe University
  - id          : 3
    institution : University of Queensland
  - id          : 4
    institution : Black Dog Institute
  - id          : 5
    institution : Curtin University
  - id          : 6
    institution : University of Wollongong

bibliography    : "references.bib"

draft           : yes
floatsintext    : yes

output: 
  prereg::prereg_pdf:
    default
  papaja::apa6_word:
    default
  github_document:
    html_preview: false

header-includes:
  - \pagenumbering{gobble}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r comment-fn}
cmt_num <- 0
word_comment <- function(highlight = "",
                         comment = "",
                         author = "Taren Sanders",
                         time = format(Sys.time(), "%Y-%m-%dT%H:%M:%SZ"),
                         id = cmt_num) {
  if (isTRUE(knitr:::pandoc_to() == "docx")) {
    cmt_str <-
      glue::glue('[{comment}]{{.comment-start id="{id}" author="{author}" \\
      date="{time}"}} {highlight} []{{.comment-end id="{id}"}}')
  } else {
    cmt_str <- glue::glue("{highlight} {{>> {comment} <<}}")
  }
  assign("cmt_num", cmt_num + 1, envir = .GlobalEnv)
  return(cmt_str)
}
```

# Study Information

## Title
<!-- Provide the working title of your study. It may be the same title that you submit for publication of your final manuscript, but it is not a requirement. The title should be a specific and informative description of a project. Vague titles such as 'Fruit fly preregistration plan' are not appropriate.

Example: Effect of sugar on brownie tastiness. -->

`r rmarkdown::metadata$title`

## Authors

```{r author-names, results='asis'}
core_team <- sapply(rmarkdown::metadata$author, function(x) x$name)
cat(paste(core_team, collapse = ", "))
```

Data contributors will be invited to co-author resulting publications (up to two authors per team).

## Description
<!-- Please give a brief description of your study, including some background, the purpose of the of the study, or broad research questions. The description should be no longer than the length of an abstract. It can give some context for the proposed study, but great detail is not needed here for your preregistration.

Example: Though there is strong evidence to suggest that sugar affects taste preferences, the effect has never been demonstrated in brownies. Therefore, we will measure taste preference for four different levels of sugar concentration in a standard brownie recipe to determine if the effect exists in this pastry. -->

This study will pool and analyze individual-level data from multiple research projects to clarify how screen time affects children's and adolescents' learning, mental health, wellbeing, and behaviour.
By uniting data from diverse samples, our team can pinpoint the specific amount of screen use (i.e., the dose) that may lead to either positive or negative outcomes, as well as how these relationships vary by characteristics such as age and gender.
By examining the type and/or content of the screen time, we can also get a better understanding of how engaging with screens may impact on children's development.
We will invite authors of relevant studies to contribute their de-identified data or share results through secure remote analysis (DataSHIELD).
After harmonising the data, piecewise regression models will be applied to identify thresholds where screen time use notably shifts from beneficial to harmful.
The findings of this IPD meta-analysis will be translated into an evidence toolkit for parents, teachers, and students.

We aim to follow answer these research questions:

1. What is the impact of screen use on children's learning, cognitive abilities, mental health, wellbeing, and behaviour?
2. Does the relationship vary by different types of screen use (e.g., content or type of device)?
3. Is there a specific duration at which notable harm/benefit becomes apparent?
4. Does the relationship/duration vary by where the screen time occurs (i.e., home vs school)?
5. Does the relationship/duration vary by characteristics of the children?

## Hypotheses
<!-- List specific, concise, and testable hypotheses. Please state if the hypotheses are directional or non-directional. If directional, state the direction. A predicted effect is also appropriate here. If a specific interaction or moderation is important to your research, you can list that as a separate hypothesis.

Example: If taste affects preference, then mean preference indices will be higher with higher concentrations of sugar. -->

We hypothesise the `r word_comment("following", "Comments or suggested changes to these hypothese are very welcome")`:

1. [RQ1] Overall screen use will have a small but statistically significant negative association with children's learning, cognitive abilities, mental health, wellbeing, and behaviour.
2. [RQ2] The type of screen time (i.e., the content or type of device) will moderate the relationship between screen use and children's outcomes.
   a. Educational content (i.e., screen time intended to educate children) will have a small-to-moderate positive associations with children's learning and cognitive abilities, but no association with mental health, wellbeing, or behaviour.
   b. Non-interactive entertainment content (e.g., television) will have a small negative association with children's learning, cognitive abilities, mental health, wellbeing, and behaviour.
   c. Interactive entertainment content (e.g., video games) will have a small negative association with children's mental health, wellbeing, and behaviour, but a negligible association with learning and cognitive abilities.
3. [RQ3] There will be a threshold of screen time at which notable harm/benefit becomes apparent, and this threshold will vary by the type of content.
4. [RQ4] The relationship between screen time and children's outcomes will not be significantly moderated by the location of the screen time (i.e., home vs school), after adjusting for content.
5. [RQ5] The relationship between screen time and children's outcomes will be moderated by characteristics of the children. Specifically:
   a. Age will moderate the relationship between screen time and children's outcomes, with `r word_comment("younger children", "I'm actually not very confident with this one. From a theory view, it seems like younger children should be more susceptible, but in the few cases where we could look at age in the umbrella review it was pretty mixed or mostly older kids. Maybe we just don't include this hypothesis.")` experiencing a stronger associations.
   b. Child gender will moderate the relationship between some forms of screen time and children's outcomes, with stronger negative effects for girls' mental health and wellbeing outcomes than boys for screen time that encourages social comparisons.

# Design Plan
<!-- In this section, you will be asked to describe the overall design of your study. Remember that this research plan is designed to register a single study, so if you have multiple experimental designs, please complete a separate preregistration. -->


## Study type

**Other** `r word_comment(comment= "Note: this is a dropdown")`

This will be an individual participant data (IPD) meta-analysis.


## Blinding
<!-- Blinding describes who is aware of the experimental manipulations within a study. Select all that apply. Is there any additional blinding in this study? -->

No blinding is involved in this study.

## Study design
<!-- Describe your study design. Examples include two-group, factorial, randomized block, and repeated measures. Is it a between (unpaired), within-subject (paired), or mixed design? Describe any counterbalancing required. Typical study designs for observation studies include cohort, cross sectional, and case-control studies.

This question has a variety of possible answers. The key is for a researcher to be as detailed as is necessary given the specifics of their design. Be careful to determine if every parameter has been specified in the description of the study design. There may be some overlap between this question and the following questions. That is OK, as long as sufficient detail is given in one of the areas to provide all of the requested information. For example, if the study design describes a complete factorial, 2 X 3 design and the treatments and levels are specified previously, you do not have to repeat that information.

Example: We have a between subjects design with 1 factor (sugar by mass) with 4 levels. -->

We will use an observational research design, using pooled data from multiple studies.
We will include both cross-sectional and longitudinal studies in the pooled analysis.

## Randomization
<!-- If you are doing a randomized study, how will you randomize, and at what level? Typical randomization techniques include: simple, block, stratified, and adaptive covariate randomization. If randomization is required for the study, the method should be specified here, not simply the source of random numbers.

Example: We will use block randomization, where each participant will be randomly assigned to one of the four equally sized, predetermined blocks. The random number list used to create these four blocks will be created using the web applications available at https://random.org. -->

There is no randomisation involved in this study.


# Sampling Plan
<!-- In this section we'll ask you to describe how you plan to collect samples, as well as the number of samples you plan to collect and your rationale for this decision. Please keep in mind that the data described in this section should be the actual data used for analysis, so if you are using a subset of a larger dataset, please describe the subset that will actually be used in your study. -->


## Existing data
<!-- Preregistration is designed to make clear the distinction between confirmatory tests, specified prior to seeing the data, and exploratory analyses conducted after observing the data. Therefore, creating a research plan in which existing data will be used presents unique challenges. Please select the description that best describes your situation. Please do not hesitate to contact us if you have questions about how to answer this question (prereg@cos.io). -->

**Registration prior to accessing the data**. As of the date of submission, the data exist, but have not been accessed by you or your collaborators. Commonly, this includes data that has been collected by another researcher or institution. `r word_comment(comment= "Note: This is a dropdown")`

## Explanation of existing data
<!-- If you indicate that you will be using some data that already exist in this study, please describe the steps you have taken to assure that you are unaware of any patterns or summary statistics in the data. This may include an explanation of how access to the data has been limited, who has observed the data, or how you have avoided observing any analysis of the specific data you will use in your study.

An appropriate instance of using existing data would be collecting a sample size much larger than is required for the study, using a small portion of it to conduct exploratory analysis, and then registering one particular analysis that showed promising results. After registration, conduct the specified analysis on that part of the dataset that had not been investigated by the researcher up to that point.

Example: An appropriate instance of using existing data would be collecting a sample size much larger than is required for the study, using a small portion of it to conduct exploratory analysis, and then registering one particular analysis that showed promising results. After registration, conduct the specified analysis on that part of the dataset that had not been investigated by the researcher up to that point. -->

We will be collating datasets from multiple existing studies on children's screen time.
The data will be de-identified, and shared with the research team either through secure transfer of data files or through secure remote analysis (DataSHIELD).
The research team may contribute their data to the pooled analysis, and therefore have prior knowledge of these data.
But, as the final analysis will be based on the pooled data, this prior knowledge does not meaningfully affect the nature of the analysis.

## Data collection procedures
<!-- Please describe the process by which you will collect your data. If you are using human subjects, this should include the population from which you obtain subjects, recruitment efforts, payment for participation, how subjects will be selected for eligibility from the initial pool (e.g. inclusion and exclusion rules), and your study timeline. For studies that donÍt include human subjects, include information about how you will collect samples, duration of data gathering efforts, source or location of samples, or batch numbers you will use.

The answer to this question requires a specific set of instructions so that another person could repeat the data collection procedures and recreate the study population. Alternatively, if the study population would be unable to be reproduced because it relies on a specific set of circumstances unlikely to be recreated (e.g., a community of people from a specific time and location), the criteria and methods for creating the group and the rationale for this unique set of subjects should be clear.

Example: Participants will be recruited through advertisements at local pastry shops. Participants will be paid $10 for agreeing to participate (raised to $30 if our sample size is not reached within 15 days of beginning recruitment). Participants must be at least 18 years old and be able to eat the ingredients of the pastries. -->

Data collection for this project will occur in two stages: one for identifying potential datasets and another for collating and harmonising the data.

### Identifying datasets

We will identify potentially relevant datasets in two ways:

1. We will examine the included studies of relevant meta-analyses, using our recent umbrella review [@sandersUmbrellaReviewBenefits2023] to identify these meta-analyses.
2. Where these meta-analyses are dated, or where a relevant meta-analysis is not identified, we will conduct a rapid review of the literature to identify relevant studies.

#### Dataset eligibility criteria

To be included in the pooled analysis, datasets must meet the following criteria:

1. Have quantitatively measured screen time exposure.
   Given the increasing evidence that the content of screen time is perhaps the most important factor in determining impact, we will only include studies that have a disaggregate measure of screen time (i.e., they have measured the content or the type as a proxy for content).
2. Have quantitatively measured at least one outcome related to children's learning, cognitive abilities, mental health, wellbeing, or behaviour. `r word_comment(comment= "This is probably too broad. Do we want to a priori pick outcomes for these?")`
3. Have a mean sample age older than 5 years and younger than 18 years.
   That is, a sample who are predominantly school-aged children and adolescents.
   If a mean study age is not available, we will use the midpoint of the age range.

#### Prioritising datasets

We expect that the process of harmonising and collating data will be very time-consuming, and the time required to complete this process will grow linearly with the number of datasets included.
Therefore, we may not be able to include all datasets that are identified, and instead need to prioritise datasets that are most likely to add value.
To do this, we will calculate the expected value of each dataset based on the following criteria:

1. The size of the sample.
2. The extent to which the dataset provides underrepresented outcomes.
3. The extent to which the dataset provides underrepresented age groups.

We will calculate the value of each dataset ($i$) as: `r word_comment(comment= "This is almost certainly over-engineered. I got a little carried away with the idea. But, I do think we need to determine how we prioritise datasets, beyond just picking the biggest ones.")`

$$
\text{Value}_i 
= 
\underbrace{\alpha \,\ln\bigl(N_i + 1\bigr)}_{\text{sample size component}}
\;+\;
\underbrace{\beta \,O_i}_{\text{outcome need component}}
\;+\;
\underbrace{\gamma \,A_i}_{\text{age need component}}
\;+\;
\underbrace{\delta \,S_i}_{\text{synergy component}},
$$

where:

- $N_i$ is the sample size of dataset $i$.
  We apply the logarithm to dampen the impact of extremely large sample sizes.
- $O_i$ (**Outcome Need**) quantifies how underrepresented the dataset's outcome is in our overall pool. For instance:

  $$
  O_i = \frac{1}{1 + \text{coverage}(\text{outcome}_i)},
  $$

  where $\text{coverage}(\text{outcome}_i)$ is the total number of participants (across the currently included datasets) that measure the same outcome.
  A larger value for $O_i$ means that the outcome is more underrepresented.
- $A_i$ (**Age Need**) captures how underrepresented the dataset's age distribution is.
  We will calculate this based on the dataset's mean age $\mu_i$ and standard deviation $\sigma_i$ by following this approach:
  1. Maintain a coverage table, $\text{Cov}(a)$, for each relevant age (or bin) $a$ of datasets already included.
  2. Approximate dataset $i$'s age distribution as a normal curve around $\mu_i$ with SD $\sigma_i$.
  3. Compute a weighted coverage:
     $$
     \text{CovWeighted}_i 
     = \sum_{a} \text{Cov}(a)\, w_i(a),
     $$
     where 
     $$
     w_i(a) 
     = \frac{\exp\!\left(-\frac{(a-\mu_i)^2}{2\,\sigma_i^2}\right)}{
         \sum_{x} \exp\!\left(-\frac{(x-\mu_i)^2}{2\,\sigma_i^2}\right)
       }.
     $$
  4. Define 
     $$
     A_i = \frac{1}{1 + \text{CovWeighted}_i}.
     $$
  This ensures $A_i$ is larger when the dataset's mean (and spread) falls in underrepresented ages.
- $S_i$ (**Synergy**) captures the fact that a dataset filling *both* an underrepresented outcome *and* an underrepresented age range is *especially* valuable.
  A common approach is to define  

  $$
  S_i = O_i \times A_i.
  $$

  Thus, $S_i$ is large if and only if *both* $O_i$ and $A_i$ are large.

Finally, $\alpha$, $\beta$, $\gamma$, and $\delta$ are *weights* that reflect how strongly we prioritise each component.
For example:

- $\alpha$ captures our emphasis on sample size,
- $\beta$ on underrepresented outcomes,
- $\gamma$ on underrepresented age ranges, and
- $\delta$ on the *interaction* of outcome and age coverage.

We will initially set these weights to $\alpha = 2$, $\beta = 1$, $\gamma = 1$, and $\delta = 2$, but may adjust these based on relative importance as data is collected.

We will then rank-order datasets based on their value, and work through the list in order of value until we reach a point where the time required to harmonise and collate the data is no longer feasible.

### Collating and harmonising data

Once datasets are identified, we will contact the corresponding authors of these studies to invite them to participate.
Authors who agree to participate will be asked to sign a letter of agreement, which will outline the terms of data sharing.
We will submit these letters of agreement to the lead institution's Human Research Ethics Committee for approval.

Once ethics approval has been granted, we will ask authors to provide their de-identified data.
We will give authors two options for sharing their data:

1. Securely sharing the de-identified raw data files with us directly. This method is less work for contributors but requires them to have ethical approval that allows for data sharing.
2. Setting up a [DataSHIELD](https://www.datashield.org/) server, an open-source solution to federated analysis where the individual-level data can be analysed remotely but without risking disclosure.
   Analysis code is sent from a central machine to each of the servers, and only non-disclosive summary statistics are returned.
   This software allows for IPDs to be conducted without accessing the data directly, which can meet the requirements of many ethics boards.

Before conducting the analysis, we will harmonise the data to ensure variables are consistent across datasets.
We will follow a process used in other federated analyses [@pinotdemoiraEUChildCohort2021].
We will pilot the harmonisation process on a subset of datasets that we have direct access to to ensure that the process is feasible and that the data can be harmonised in a meaningful way.
We will then ask data contributors who are using DataSHIELD to harmonise their data in the same way.
To validate that this has happened correctly, we will provide a script to contributors that will check that the data matches expectations.
This harmonised data can then be added to a DataShield server for analysis.


## Sample size
<!-- Describe the sample size of your study. How many units will be analyzed in the study? This could be the number of people, birds, classrooms, plots, interactions, or countries included. If the units are not individuals, then describe the size requirements for each unit. If you are using a clustered or multilevel design, how many units are you collecting at each level of the analysis? For some studies, this will simply be the number of samples or the number of clusters. For others, this could be an expected range, minimum, or maximum number.

Example: Our target sample size is 280 participants. We will attempt to recruit up to 320, assuming that not all will complete the total task. -->

Given the volume of research on children's screen time, we anticipate that we will be able to include a large number of datasets in the pooled analysis, and therefore are not concerned about the number of participants.

## Sample size rationale
<!-- This could include a power analysis or an arbitrary constraint such as time, money, or personnel. This gives you an opportunity to specifically state how the sample size will be determined. A wide range of possible answers is acceptable; remember that transparency is more important than principled justifications. If you state any reason for a sample size upfront, it is better than stating no reason and leaving the reader to "fill in the blanks." Acceptable rationales include: a power analysis, an arbitrary number of subjects, or a number based on time or monetary constraints.

Example: We used the software program G*Power to conduct a power analysis. Our goal was to obtain .95 power to detect a medium effect size of .25 at the standard .05 alpha error probability. -->

Given that we expect to recruit a very large sample, we are not concerned about statistical power.

## Stopping rule
<!-- If your data collection procedures do not give you full control over your exact sample size, specify how you will decide when to terminate your data collection. 

You may specify a stopping rule based on p-values only in the specific case of sequential analyses with pre-specified checkpoints, alphas levels, and stopping rules. Unacceptable rationales include stopping based on p-values if checkpoints and stopping rules are not specified. If you have control over your sample size, then including a stopping rule is not necessary, though it must be clear in this question or a previous question how an exact sample size is attained.

Example: We will post participant sign-up slots by week on the preceding Friday night, with 20 spots posted per week. We will post 20 new slots each week if, on that Friday night, we are below 320 participants. -->

We will create a version of the dataset in March 2026 to be used for this registered analysis.
However, as we intend to conduct further analyses on this dataset in the future, we will allow additional datasets to be added to the analytical sample after this date.

# Variables
<!-- In this section you can describe all variables (both manipulated and measured variables) that will later be used in your confirmatory analysis plan. In your analysis plan, you will have the opportunity to describe how each variable will be used. If you have variables which you are measuring for exploratory analyses, you are not required to list them, though you are permitted to do so. -->


## Manipulated variables
<!-- Describe all variables you plan to manipulate and the levels or treatment arms of each variable. This is not applicable to any observational study. For any experimental manipulation, you should give a precise definition of each manipulated variable. This must include a precise description of the levels at which each variable will be set, or a specific definition for each categorical treatment. For example, “loud or quiet,” should instead give either a precise decibel level or a means of recreating each level. 'Presence/absence' or 'positive/negative' is an acceptable description if the variable is precisely described.

Example: We manipulated the percentage of sugar by mass added to brownies. The four levels of this categorical variable are: 15%, 20%, 25%, or 40% cane sugar by mass. -->

There are no manipulated variables in this study.

## Measured variables
<!-- Describe each variable that you will measure. This will include outcome measures, as well as any predictors or covariates that you will measure. You do not need to include any variables that you plan on collecting if they are not going to be included in the confirmatory analyses of this study.

Observational studies and meta-analyses will include only measured variables. As with the previous questions, the answers here must be precise. For example, 'intelligence,' 'accuracy,' 'aggression,' and 'color' are too vague. Acceptable alternatives could be 'IQ as measured by Wechsler Adult Intelligence Scale' 'percent correct,' 'number of threat displays,' and 'percent reflectance at 400 nm.'

Example: The single outcome variable will be the perceived tastiness of the single brownie each participant will eat. We will measure this by asking participants ‘How much did you enjoy eating the brownie' (on a scale of 1-7, 1 being 'not at all', 7 being 'a great deal') and 'How good did the brownie taste' (on a scale of 1-7, 1 being 'very bad', 7 being 'very good'). -->

The exact variables that will be measured will depend on the datasets that are included in the pooled analysis, and the extent to which they are able to be harmonised.
However we expect to include at least the following variables:

### Measures of screen use

While there is no consensus or standard tool for measuring screen, several survey tools have gained popularity in the literature. `r word_comment(comment= "For team to consider: should we be including studies that use time use diaries? This would make harmonising more difficult, but lots of studies have used MARCA etc as their measure.")`
These include the Screen Based Media Use Scale [@houghtonVirtuallyImpossibleLimiting2015], and Youth Risk Behavior Survey [@schmitzReliabilityValidityBrief2004], and time use diary methods such as the Multimedia Activity Recall for Children and Adolescents [@ridleyMultimediaActivityRecall2006].
From these, we can predict some of the measures we expect to be included in the pooled dataset.

* **Total screen time**: As an aggregated measure of screen time.
  We expect most studies to have already calculated this value, but if not, we will calculate it as the sum of time spent on different devices or types.
* **Video game**: Time spent playing video games.
* **Television**: Time spent watching television.
* **Mobile device**: Time spent on mobile devices, such as phones and tablets.
* **Social media**: Time spent on social media.
* **Computer**: Time spent on computers.
* **Educational time**: Time spent on using devices for educational purposes, such as to complete homework.

We will harmonise all measures of screen use to a common unit (average hours per day).
In addition, we will record the the tool used to `r word_comment("test", "or adjust?")` for systematic differences across tools.

Note that we will not include measures which only indicate 'problematic' screen use, or have only a dichotomous measure of screen use (e.g., 'meets guidelines' or 'does not meet guidelines').

### Outcome measures

`r word_comment(comment= "Outcomes are the part of this I am most concerned about. We need to balance the extent to which we can meaningfully combine measures, with the extent to which we can reasonably expect to find datasets. E.g., if we limit 'beahviour' to the SDQ, we may not find enough datasets. But if we include all measures of behaviour, we may not be able to meaningfully combine them. Thoughts on how we address this are welcome.")`
We will include a range of outcome measures related to children's learning, cognitive abilities, mental health, wellbeing, and behaviour.
After identifying datasets, we will examine the measures used in these datasets and determine which measures can be harmonised and have sufficient data before contacting authors.

The below outline some of the measures we expect to be included.

* **Learning**: Measures of academic performance, such as standardised test scores, grades, or teacher ratings.
* **Cognitive abilities**: Measures of cognitive function, executive function, or memory.
* **Mental health**: Measures of mental health, such as measures of depression and anxiety.
* **Behaviour**: Measures of behavioural problems in children, such as the Strengths and Difficulties Questionnaire [@goodmanStrengthsDifficultiesQuestionnaire1997], or the Child Behaviour Checklist [@achenbachManualASEBASchoolage2001] `r word_comment(".", "Would be good to include some positive behaviour measures (prosociality) too, if feasible.")`
* **Wellbeing**: Measures of subjective `r word_comment("wellbeing", "As always, I'm not really confident on what wellbeing really means. We might consider dropping it.")`.

### Covariates and moderators of effects

We will also ask authors to provide data on a range of covariates and moderators that may influence the relationship between screen time and children's outcomes, if they were measured in the study.
These include:

* Child demographics, such as age, gender, and ethnicity.
* Socioeconomic status, such as parental education and income.
* Location of screen time, such as home or school.

## Indices
<!-- If any measurements are  going to be combined into an index (or even a mean), what measures will you use and how will they be combined? Include either a formula or a precise description of your method. If your are using a more complicated statistical method to combine measures (e.g. a factor analysis), you can note that here but describe the exact method in the analysis plan section.

If you are using multiple pieces of data to construct a single variable, how will this occur? Both the data that are included and the formula or weights for each measure must be specified. Standard summary statistics, such as "means" do not require a formula, though more complicated indices require either the exact formula or, if it is an established index in the field, the index must be unambiguously defined. For example, "biodiversity index" is too broad, whereas "Shannon's biodiversity index" is appropriate.

Example: We will take the mean of the two questions above to create a single measure of 'brownie enjoyment.'  -->

The nature of this study makes it hard to predict which measures will be combined in an index, beyond aggregated total screen time.
However, we will publish a codebook which includes the variables and how to create them as part of the harmonisation process, which will be prior to analysis.


# Analysis Plan
<!-- You may describe one or more confirmatory analysis in this preregistration. Please remember that all analyses specified below must be reported in the final article, and any additional analyses must be noted as exploratory or hypothesis generating.

A confirmatory analysis plan must state up front which variables are predictors (independent) and which are the outcomes (dependent), otherwise it is an exploratory analysis. You are allowed to describe any exploratory work here, but a clear confirmatory analysis is required. -->


## Statistical models
<!-- What statistical model will you use to test each hypothesis? Please include the type of model (e.g. ANOVA, multiple regression, SEM, etc) and the specification of the model (this includes each variable that will be included as predictors, outcomes, or covariates). Please specify any interactions, subgroup analyses, pairwise or complex contrasts, or follow-up tests from omnibus tests. If you plan on using any positive controls, negative controls, or manipulation checks you may mention that here. Remember that any test not included here must be noted as an exploratory test in your final article.

This is perhaps the most important and most complicated question within the preregistration. As with all of the other questions, the key is to provide a specific recipe for analyzing the collected data. Ask yourself: is enough detail provided to run the same analysis again with the information provided by the user? Be aware for instances where the statistical models appear specific, but actually leave openings for the precise test. See the following examples:

- If someone specifies a 2x3 ANOVA with both factors within subjects, there is still flexibility with the various types of ANOVAs that could be run. Either a repeated measures ANOVA (RMANOVA) or a multivariate ANOVA (MANOVA) could be used for that design, which are two different tests. 
- If you are going to perform a sequential analysis and check after 50, 100, and 150 samples, you must also specify the p-values you'll test against at those three points.

Example:  We will use a one-way between subjects ANOVA to analyze our results. The manipulated, categorical independent variable is 'sugar' whereas the dependent variable is our taste index. -->

Enter your response here.


## Transformations
<!-- If you plan on transforming, centering, recoding the data, or will require a coding scheme for categorical variables, please describe that process. If any categorical predictors are included in a regression, indicate how those variables will be coded (e.g. dummy coding, summation coding, etc.) and what the reference category will be.

Example: The "Effect of sugar on brownie tastiness" does not require any additional transformations. However, if it were using a regression analysis and each level of sweet had been categorically described (e.g. not sweet, somewhat sweet, sweet, and very sweet), 'sweet' could be dummy coded with 'not sweet' as the reference category. -->

Enter your response here.


## Inference criteria
<!-- What criteria will you use to make inferences? Please describe the information youÍll use (e.g. p-values, bayes factors, specific model fit indices), as well as cut-off criterion, where appropriate. Will you be using one or two tailed tests for each of your analyses? If you are comparing multiple conditions or testing multiple hypotheses, will you account for this?

p-values, confidence intervals, and effect sizes are standard means for making an inference, and any level is acceptable, though some criteria must be specified in this or previous fields. Bayesian analyses should specify a Bayes factor or a credible interval. If you are selecting models, then how will you determine the relative quality of each? In regards to multiple comparisons, this is a question with few "wrong" answers. In other words, transparency is more important than any specific method of controlling the false discovery rate or false error rate. One may state an intention to report all tests conducted or one may conduct a specific correction procedure; either strategy is acceptable.

Example: We will use the standard p<.05 criteria for determining if the ANOVA and the post hoc test suggest that the results are significantly different from those expected if the null hypothesis were correct. The post-hoc Tukey-Kramer test adjusts for multiple comparisons. -->


## Data exclusion
<!-- How will you determine what data or samples, if any, to exclude from your analyses? How will outliers be handled? Will you use any awareness check? Any rule for excluding a particular set of data is acceptable. One may describe rules for excluding a participant or for identifying outlier data.

Example: No checks will be performed to determine eligibility for inclusion besides verification that each subject answered each of the three tastiness indices. Outliers will be included in the analysis. -->

Enter your response here.


## Missing data
<!-- How will you deal with incomplete or missing data? Any relevant explanation is acceptable. As a final reminder, remember that the final analysis must follow the specified plan, and deviations must be either strongly justified or included as a separate, exploratory analysis.

Example: If a subject does not complete any of the three indices of tastiness, that subject will not be included in the analysis. -->

Enter your response here.


## Exploratory analyses (optional)
<!-- If you plan to explore your data set to look for unexpected differences or relationships, you may describe those tests here. An exploratory test is any test where a prediction is not made up front, or there are multiple possible tests that you are going to use. A statistically significant finding in an exploratory test is a great way to form a new confirmatory hypothesis, which could be registered at a later time.

Example: We expect that certain demographic traits may be related to taste preferences. Therefore, we will look for relationships between demographic variables (age, gender, income, and marital status) and the primary outcome measures of taste preferences. -->

Enter your response here.


# Other

## Other (Optional)
<!-- If there is any additional information that you feel needs to be included in your preregistration, please enter it here. Literature cited, disclosures of any related work such as replications or work that uses the same data, or other context that will be helpful for future readers would be appropriate here. -->

Enter your response here.

\newpage

# References
## 
\vspace{-2pc}
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{-1in}
\setlength{\parskip}{8pt}
\noindent
